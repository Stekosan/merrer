{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stekosan/merrer/blob/main/Copy_of_SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ46j-PD-eXS"
      },
      "source": [
        "Let's set up SparkNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz8walkjXq0I",
        "outputId": "9611af19-4270-44a8-80fa-ce469d564bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-27 06:36:01--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 3.86.22.73\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|3.86.22.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2024-07-27 06:36:01--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-27 06:36:01 (50.4 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.4.1\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.4.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.2/579.2 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8trkawwfX8bx"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSv2iBclzWMr",
        "outputId": "99d2283c-659c-4cca-824f-700b6a224068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "explain_document_ml download started this may take some time.\n",
            "Approx size to download 9 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "pipeline = PretrainedPipeline(\"explain_document_ml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzvSSJOQ-kKm"
      },
      "source": [
        "We can use some recent headlines."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hls = [ # was headlines\n",
        "\t\t\"She\",\n",
        "\t\t\"He\",\n",
        "\t\t\"her\",\n",
        "\t\t\"him\",\n",
        "\t\t\"hers\",\n",
        "\t\t\"his\"\n",
        "\t]"
      ],
      "metadata": {
        "id": "VC-21qMEpLWM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaexWg9g-oJJ"
      },
      "source": [
        "Let's use SparkNLP to analyze these headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FkD7QdVMZPjT"
      },
      "outputs": [],
      "source": [
        "# Use dataframes, or...\n",
        "# data = spark.createDataFrame(hls).toDF(\"text\")\n",
        "# dfs = pipeline.transform(data)\n",
        "# ... use list comprehension\n",
        "dfs = [pipeline.annotate(hl) for hl in hls] # I don't know how to use dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xWvlqNUSYxjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99af1a96-369e-48e7-afe0-3f41337acd79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': ['She'],\n",
              "  'spell': ['She'],\n",
              "  'pos': ['PRP'],\n",
              "  'lemmas': ['She'],\n",
              "  'token': ['She'],\n",
              "  'stems': ['she'],\n",
              "  'sentence': ['She']},\n",
              " {'document': ['He'],\n",
              "  'spell': ['He'],\n",
              "  'pos': ['PRP'],\n",
              "  'lemmas': ['He'],\n",
              "  'token': ['He'],\n",
              "  'stems': ['he'],\n",
              "  'sentence': ['He']},\n",
              " {'document': ['her'],\n",
              "  'spell': ['her'],\n",
              "  'pos': ['PRP$'],\n",
              "  'lemmas': ['she'],\n",
              "  'token': ['her'],\n",
              "  'stems': ['her'],\n",
              "  'sentence': ['her']},\n",
              " {'document': ['him'],\n",
              "  'spell': ['him'],\n",
              "  'pos': ['PRP'],\n",
              "  'lemmas': ['he'],\n",
              "  'token': ['him'],\n",
              "  'stems': ['him'],\n",
              "  'sentence': ['him']},\n",
              " {'document': ['hers'],\n",
              "  'spell': ['hers'],\n",
              "  'pos': ['NNS'],\n",
              "  'lemmas': ['hers'],\n",
              "  'token': ['hers'],\n",
              "  'stems': ['her'],\n",
              "  'sentence': ['hers']},\n",
              " {'document': ['his'],\n",
              "  'spell': ['his'],\n",
              "  'pos': ['PRP$'],\n",
              "  'lemmas': ['he'],\n",
              "  'token': ['his'],\n",
              "  'stems': ['hi'],\n",
              "  'sentence': ['his']}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# its big\n",
        "dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbYI4VqB1JCh"
      },
      "source": [
        "Let's say we want to fuse part-of-speech tags to words, to make word differentiation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ObEtMFe2aFTr"
      },
      "outputs": [],
      "source": [
        "# Extract words and parts-of-speech\n",
        "tok_tag = [(df['token'],df['pos']) for df in dfs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DjmzGUa9au-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3595644f-0afa-41e5-95dd-78d6e6696b1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['She'], ['PRP']),\n",
              " (['He'], ['PRP']),\n",
              " (['her'], ['PRP$']),\n",
              " (['him'], ['PRP']),\n",
              " (['hers'], ['NNS']),\n",
              " (['his'], ['PRP$'])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Still big\n",
        "tok_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_quMwObV253r"
      },
      "outputs": [],
      "source": [
        "# fuse pos to word\n",
        "zips = [list(zip(tt[0], tt[1])) for tt in tok_tag]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OhtChnE3s0R",
        "outputId": "7f1a0ef4-27e1-4a2a-bb1a-115087dabcf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('She', 'PRP')],\n",
              " [('He', 'PRP')],\n",
              " [('her', 'PRP$')],\n",
              " [('him', 'PRP')],\n",
              " [('hers', 'NNS')],\n",
              " [('his', 'PRP$')]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# not too big\n",
        "zips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "plqXGLJ04aVC"
      },
      "outputs": [],
      "source": [
        "tagged = [\" \".join([\"\".join(word) for word in hl]) for hl in zips]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs45yaMK4nn-",
        "outputId": "b0d59ad5-6d15-446a-c1d4-4f84f9d40c5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ShePRP', 'HePRP', 'herPRP$', 'himPRP', 'hersNNS', 'hisPRP$']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVnTs4mW5Acm"
      },
      "source": [
        "What about ebooks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDk3Kjxa_ZQC",
        "outputId": "9ff18221-b0ff-4fb7-b472-38721ad5a336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  739k  100  739k    0     0  2135k      0 --:--:-- --:--:-- --:--:-- 2138k\n"
          ]
        }
      ],
      "source": [
        "!curl \"https://raw.githubusercontent.com/cd-public/books/main/pg1342.txt\" -o austen.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yRwsiMoK43bU"
      },
      "outputs": [],
      "source": [
        "austen = open('austen.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wlNlWDz5RPC",
        "outputId": "38620060-813b-4a3e-a266-d335a26eec1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Pride and Prejudice\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: Pride and Prejudice\n",
            "\n",
            "Author: Jane Austen\n",
            "\n",
            "Release date: June 1, 1998 [eBook #1342]\n",
            "                Most recently updated: April 14, 2023\n",
            "\n",
            "Language: English\n",
            "\n",
            "Credits: Chuck Greif and the Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from images available at The Internet Archive)\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            [Illustration:\n",
            "\n",
            "                             GEORGE ALLEN\n",
            "                 \n"
          ]
        }
      ],
      "source": [
        "print(austen[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu7LjNBx91G2",
        "outputId": "19c68f73-db36-47ab-a8e8-0924769172e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'CC',\n",
              " 'NNP',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'RB']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "pipeline.annotate(austen[:100])['pos']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4AkPuc-dLL"
      },
      "source": [
        "Previously with ebooks, we conducted word counts. We can do that here as well, with Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eTyhjm5c_OZ-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0umuUz1H_PaB"
      },
      "outputs": [],
      "source": [
        "# change 'austen' variable from a string to a spark object\n",
        "austen = spark.sparkContext.textFile(\"austen.txt\")\n",
        "\n",
        "counts = (\n",
        "    austen.flatMap(lambda line: line.split(\" \"))\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzfDO41G_4Yv",
        "outputId": "88202d8e-83a1-4380-9ab4-b08082c58898"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 285),\n",
              " ('Project', 79),\n",
              " ('of', 3897),\n",
              " ('Pride', 7),\n",
              " ('', 10603),\n",
              " ('ebook', 2),\n",
              " ('is', 861),\n",
              " ('use', 23),\n",
              " ('anyone', 20),\n",
              " ('anywhere', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "counts.collect()[:10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}