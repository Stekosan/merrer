{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stekosan/merrer/blob/main/Copy_of_SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ46j-PD-eXS"
      },
      "source": [
        "Let's set up SparkNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz8walkjXq0I",
        "outputId": "4a823b4e-e14d-43fb-a0be-8fe847dff0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-27 07:31:15--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 3.86.22.73\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|3.86.22.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2024-07-27 07:31:15--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 5.4.1\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-27 07:31:15 (46.8 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.4.1\n"
          ]
        }
      ],
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8trkawwfX8bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9feb21ca-907f-45b4-d607-3b34fb709808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSv2iBclzWMr",
        "outputId": "e082f3d7-ba20-4fc2-c211-69dac5544175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "explain_document_ml download started this may take some time.\n",
            "Approx size to download 9 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "pipeline = PretrainedPipeline(\"explain_document_ml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzvSSJOQ-kKm"
      },
      "source": [
        "We can use some recent headlines."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hls = [ # was headlines\n",
        "\t\t\"She\",\n",
        "\t\t\"He\",\n",
        "\t\t\"her\",\n",
        "\t\t\"him\",\n",
        "\t\t\"hers\",\n",
        "\t\t\"his\"\n",
        "\t]"
      ],
      "metadata": {
        "id": "VC-21qMEpLWM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaexWg9g-oJJ"
      },
      "source": [
        "Let's use SparkNLP to analyze these headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FkD7QdVMZPjT"
      },
      "outputs": [],
      "source": [
        "# Use dataframes, or...\n",
        "# data = spark.createDataFrame(hls).toDF(\"text\")\n",
        "# dfs = pipeline.transform(data)\n",
        "# ... use list comprehension\n",
        "dfs = [pipeline.annotate(hl) for hl in hls] # I don't know how to use dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xWvlqNUSYxjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb0d352-71a1-4e84-a75a-944627089f28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': ['She'],\n",
              "  'spell': ['She'],\n",
              "  'pos': ['PRP'],\n",
              "  'lemmas': ['She'],\n",
              "  'token': ['She'],\n",
              "  'stems': ['she'],\n",
              "  'sentence': ['She']},\n",
              " {'document': ['He'],\n",
              "  'spell': ['He'],\n",
              "  'pos': ['PRP'],\n",
              "  'lemmas': ['He'],\n",
              "  'token': ['He'],\n",
              "  'stems': ['he'],\n",
              "  'sentence': ['He']},\n",
              " {'document': ['her'],\n",
              "  'spell': ['her'],\n",
              "  'pos': ['PRP$'],\n",
              "  'lemmas': ['she'],\n",
              "  'token': ['her'],\n",
              "  'stems': ['her'],\n",
              "  'sentence': ['her']},\n",
              " {'document': ['him'],\n",
              "  'spell': ['him'],\n",
              "  'pos': ['PRP'],\n",
              "  'lemmas': ['he'],\n",
              "  'token': ['him'],\n",
              "  'stems': ['him'],\n",
              "  'sentence': ['him']},\n",
              " {'document': ['hers'],\n",
              "  'spell': ['hers'],\n",
              "  'pos': ['NNS'],\n",
              "  'lemmas': ['hers'],\n",
              "  'token': ['hers'],\n",
              "  'stems': ['her'],\n",
              "  'sentence': ['hers']},\n",
              " {'document': ['his'],\n",
              "  'spell': ['his'],\n",
              "  'pos': ['PRP$'],\n",
              "  'lemmas': ['he'],\n",
              "  'token': ['his'],\n",
              "  'stems': ['hi'],\n",
              "  'sentence': ['his']}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# its big\n",
        "dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbYI4VqB1JCh"
      },
      "source": [
        "Let's say we want to fuse part-of-speech tags to words, to make word differentiation easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ObEtMFe2aFTr"
      },
      "outputs": [],
      "source": [
        "# Extract words and parts-of-speech\n",
        "tok_tag = [(df['token'],df['pos']) for df in dfs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DjmzGUa9au-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a4b27f-ae0f-447d-a2d3-9bba5f9d2106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['She'], ['PRP']),\n",
              " (['He'], ['PRP']),\n",
              " (['her'], ['PRP$']),\n",
              " (['him'], ['PRP']),\n",
              " (['hers'], ['NNS']),\n",
              " (['his'], ['PRP$'])]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Still big\n",
        "tok_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_quMwObV253r"
      },
      "outputs": [],
      "source": [
        "# fuse pos to word\n",
        "zips = [list(zip(tt[0], tt[1])) for tt in tok_tag]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OhtChnE3s0R",
        "outputId": "2c13eba9-01be-4c61-c3da-58d592b6c30e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('She', 'PRP')],\n",
              " [('He', 'PRP')],\n",
              " [('her', 'PRP$')],\n",
              " [('him', 'PRP')],\n",
              " [('hers', 'NNS')],\n",
              " [('his', 'PRP$')]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# not too big\n",
        "zips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "plqXGLJ04aVC"
      },
      "outputs": [],
      "source": [
        "tagged = [\" \".join([\"\".join(word) for word in hl]) for hl in zips]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs45yaMK4nn-",
        "outputId": "c10b9030-54f7-49fb-89f8-7b9456446e24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ShePRP', 'HePRP', 'herPRP$', 'himPRP', 'hersNNS', 'hisPRP$']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "tagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVnTs4mW5Acm"
      },
      "source": [
        "What about ebooks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDk3Kjxa_ZQC",
        "outputId": "6110f16a-673e-4e65-87d0-f1437885f8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  325k  100  325k    0     0   582k      0 --:--:-- --:--:-- --:--:--  581k\n"
          ]
        }
      ],
      "source": [
        "!curl \"https://raw.githubusercontent.com/Stekosan/merrer/main/sparknlp/Diaries_of_Court_Ladies_of_Old_Japan.txt\" -o diary.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "yRwsiMoK43bU"
      },
      "outputs": [],
      "source": [
        "diary = open('diary.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wlNlWDz5RPC",
        "outputId": "5ad29787-ac3b-4e4d-a37a-329884a420ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿*** START OF THE PROJECT GUTENBERG EBOOK DIARIES OF COURT LADIES OF OLD JAPAN ***\n",
            "\n",
            "DIARIES OF\n",
            "\n",
            "COURT LADIES OF OLD JAPAN\n",
            "\n",
            "TRANSLATED BY\n",
            "\n",
            "ANNIE SHEPLEY OMORI\n",
            "\n",
            "AND\n",
            "\n",
            "KOCHI DOI\n",
            "\n",
            "_Professor in the Imperial University, Tokio_\n",
            "\n",
            "WITH AN INTRODUCTION BY\n",
            "\n",
            "AMY LOWELL\n",
            "\n",
            "\n",
            "_And with Illustrations_\n",
            "\n",
            "\n",
            "BOSTON AND NEW YORK\n",
            "\n",
            "HOUGHTON MIFFLIN COMPANY\n",
            "\n",
            "The Riverside Press Cambridge\n",
            "\n",
            "1920\n",
            "\n",
            "\n",
            "\n",
            "[Illustration: COURT LADY'S FULL DRESS IN THE HEIAN PERIOD]\n",
            "\n",
            "(For explanation see List of Illustrations)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TRANSLATORS' NOTE\n",
            "\n",
            "\n",
            "The poems in the text, slight and occasional as they are, depending\n",
            "often for their charm on plays upon words of two meanings, or on\n",
            "the suggestions conveyed to the Japanese mind by a single word,\n",
            "have presented problems of great difficulty to the translators, not\n",
            "perfectly overcome.\n",
            "\n",
            "Izumi Shikibu's Diary is written with extreme delicacy of treatment.\n",
            "English words and thought seem too downright a medium into which to\n",
            "render these evanescent, half-expressed sentences and poems--vague as\n",
            "the m\n"
          ]
        }
      ],
      "source": [
        "print(diary[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu7LjNBx91G2",
        "outputId": "956a1d7d-c03b-48af-aa56-0c01fbb65378"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NN',\n",
              " 'NN',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'PRP$']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "pipeline.annotate(diary[:100])['pos']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH4AkPuc-dLL"
      },
      "source": [
        "Previously with ebooks, we conducted word counts. We can do that here as well, with Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "eTyhjm5c_OZ-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0umuUz1H_PaB"
      },
      "outputs": [],
      "source": [
        "# change 'austen' variable from a string to a spark object\n",
        "diary = spark.sparkContext.textFile(\"diary.txt\")\n",
        "\n",
        "counts = (\n",
        "    diary.flatMap(lambda line: line.split(\" \"))\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzfDO41G_4Yv",
        "outputId": "0a6cec34-141d-46b1-f0f7-86ef1252f9a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('***', 4),\n",
              " ('OF', 23),\n",
              " ('GUTENBERG', 2),\n",
              " ('DIARIES', 5),\n",
              " ('COURT', 8),\n",
              " ('LADIES', 4),\n",
              " ('OLD', 7),\n",
              " ('JAPAN', 4),\n",
              " ('', 7244),\n",
              " ('TRANSLATED', 1),\n",
              " ('ANNIE', 1),\n",
              " ('SHEPLEY', 1),\n",
              " ('OMORI', 1),\n",
              " ('KOCHI', 1),\n",
              " ('_Professor', 1),\n",
              " ('in', 922),\n",
              " ('Imperial', 17),\n",
              " ('University,', 1),\n",
              " ('Tokio_', 1),\n",
              " ('INTRODUCTION', 3),\n",
              " ('LOWELL', 3),\n",
              " ('Illustrations_', 1),\n",
              " ('NEW', 1),\n",
              " ('YORK', 1),\n",
              " ('HOUGHTON', 1),\n",
              " ('MIFFLIN', 1),\n",
              " ('The', 506),\n",
              " ('Riverside', 1),\n",
              " ('Cambridge', 1),\n",
              " ('1920', 1),\n",
              " (\"LADY'S\", 2),\n",
              " ('FULL', 2),\n",
              " ('HEIAN', 2),\n",
              " ('List', 2),\n",
              " ('of', 1803),\n",
              " ('NOTE', 1),\n",
              " ('poems', 28),\n",
              " ('text,', 2),\n",
              " ('occasional', 5),\n",
              " ('as', 247),\n",
              " ('are,', 3),\n",
              " ('depending', 1),\n",
              " ('plays', 1),\n",
              " ('upon', 35),\n",
              " ('two', 51),\n",
              " ('conveyed', 1),\n",
              " ('mind', 46),\n",
              " ('single', 3),\n",
              " ('have', 213),\n",
              " ('presented', 17),\n",
              " ('difficulty', 3),\n",
              " ('perfectly', 3),\n",
              " ('overcome.', 1),\n",
              " ('is', 547),\n",
              " ('extreme', 5),\n",
              " ('delicacy', 2),\n",
              " ('treatment.', 1),\n",
              " ('English', 2),\n",
              " ('thought', 84),\n",
              " ('downright', 1),\n",
              " ('medium', 2),\n",
              " ('into', 74),\n",
              " ('render', 1),\n",
              " ('these', 80),\n",
              " ('evanescent,', 1),\n",
              " ('half-expressed', 1),\n",
              " ('misty', 2),\n",
              " ('her', 364),\n",
              " ('country,', 7),\n",
              " ('no', 148),\n",
              " ('at', 278),\n",
              " ('verb', 1),\n",
              " ('shy', 7),\n",
              " ('reserve', 4),\n",
              " ('record', 5),\n",
              " ('induced', 1),\n",
              " ('use', 4),\n",
              " ('third', 14),\n",
              " ('person', 34),\n",
              " ('best', 7),\n",
              " ('means', 16),\n",
              " ('it.', 58),\n",
              " ('Of', 7),\n",
              " ('\"Sarashina', 9),\n",
              " ('there', 149),\n",
              " ('three', 39),\n",
              " ('four', 16),\n",
              " ('publications', 1),\n",
              " ('them', 88),\n",
              " ('are', 229),\n",
              " ('confused', 2),\n",
              " ('unreadably', 1),\n",
              " ('was', 937),\n",
              " ('texts', 1),\n",
              " ('especially', 10),\n",
              " ('Mr.', 3),\n",
              " (\"Girls'\", 1),\n",
              " ('Normal', 1),\n",
              " ('School,', 1),\n",
              " ('published', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "counts.collect()[:100]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}